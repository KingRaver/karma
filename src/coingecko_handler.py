#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸš€ ENHANCED COINGECKO HANDLER V3.0 ðŸš€
===============================================================================

NEXT-GENERATION COINGECKO API INTEGRATION
Built for complex trading systems with advanced data validation
100% backward compatible with existing bot.py and prediction_engine.py

ENHANCED FEATURES:
âœ… Advanced error handling and recovery mechanisms
âœ… M4 optimization support
âœ… Intelligent caching with data quality tracking
âœ… Rate limiting with smart retry strategies
âœ… Integration with technical analysis systems
âœ… Real-time data quality monitoring

Author: Ted Khao
Version: 3.0 - Production Ready Edition
Compatible with: All existing bot.py and prediction_engine.py calls
"""

import time
import requests
import json
import math
import hashlib
from typing import Dict, List, Optional, Any, Union, Set, Tuple
from datetime import datetime, timedelta
from datetime_utils import strip_timezone, ensure_naive_datetimes, safe_datetime_diff
from utils.logger import logger

# ============================================================================
# ðŸŽ¯ ENHANCED DATA VALIDATION SYSTEM ðŸŽ¯
# ============================================================================

class DataQualityValidator:
    """
    ðŸ” REAL DATA VALIDATION SYSTEM ðŸ”
    
    Validates only real market data - no synthetic generation
    Returns empty arrays for invalid/missing sparkline data
    """
    
    @staticmethod
    def validate_sparkline_data(sparkline_data: Any) -> Tuple[bool, List[float]]:
        """
        Validate and extract REAL price data from CoinGecko sparkline only
        NO SYNTHETIC DATA GENERATION
        
        Returns:
            (is_valid, price_array) - price_array is empty if invalid
        """
        try:
            # Handle various sparkline formats from CoinGecko
            if not sparkline_data:
                return False, []
            
            # Extract price array based on CoinGecko format
            price_array = []
            
            if isinstance(sparkline_data, dict):
                if 'price' in sparkline_data:
                    price_array = sparkline_data['price']
                elif 'prices' in sparkline_data:
                    price_array = sparkline_data['prices']
            elif isinstance(sparkline_data, list):
                price_array = sparkline_data
            else:
                return False, []
            
            # Validate price array - REAL data only
            if not price_array or len(price_array) < 10:
                return False, []
            
            # Clean and validate individual prices
            valid_prices = []
            for price in price_array:
                try:
                    if price is not None and isinstance(price, (int, float)):
                        float_price = float(price)
                        if math.isfinite(float_price) and float_price > 0:
                            valid_prices.append(float_price)
                except:
                    continue
            
            # Require at least 10 valid prices for technical analysis
            if len(valid_prices) < 10:
                return False, []
            
            return True, valid_prices
            
        except Exception as e:
            logger.error(f"Sparkline validation error: {e}")
            return False, []

# ============================================================================
# ðŸš€ ENHANCED COINGECKO HANDLER CLASS ðŸš€
# ============================================================================

class CoinGeckoHandler:
    """
    ðŸš€ NEXT-GENERATION COINGECKO API HANDLER ðŸš€
    
    Enhanced for complex trading systems with advanced features:
    - Robust data validation and quality assurance
    - Synthetic data generation for reliability
    - M4 optimization support with fallbacks
    - Advanced caching and rate limiting
    - Real-time monitoring and error recovery
    """
    
    def __init__(self, base_url: str, cache_duration: int = 60) -> None:
        """
        Initialize the enhanced CoinGecko handler
        
        Args:
            base_url: The base URL for the CoinGecko API
            cache_duration: Cache duration in seconds
        """
        self.base_url = base_url
        self.cache_duration = cache_duration
        self.cache = {}
        self.last_request_time = 0
        self.min_request_interval = 1.2  # Optimized for modern APIs

        # Enhanced headers
        self.user_agent = 'CryptoBot/1.0 (Python/requests)'
        
        # Enhanced tracking
        self.daily_requests = 0
        self.daily_requests_reset = datetime.now()
        self.failed_requests = 0
        self.successful_requests = 0
        self.active_retries = 0
        self.max_retries = 3
        self.retry_delay = 3  # Reduced for faster recovery
        
        # Data quality tracking
        self.data_quality_stats = {
            'valid_sparklines': 0,
            'failed_validations': 0,
            'total_requests': 0,
            'cache_hits': 0,
            'optimization_runs': 0
        }
        
        # Enhanced headers for better API compatibility - Industry best practice user agent
        self.user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        
        # Token ID cache for performance
        self.token_id_cache = {}
        
        # Data validator instance
        self.validator = DataQualityValidator()
        
        # Initialize TokenMappingManager for dynamic token management (Industry best practice - no hardcoded tokens)
        try:
            from config import TokenMappingManager
            self.token_mapper = TokenMappingManager()
            logger.info("âœ… TokenMappingManager initialized")
            
            # Validate TokenMappingManager has sufficient tokens for operations
            available_tokens = self.token_mapper.get_all_available_tokens(include_database=True)
            if not available_tokens or len(available_tokens.get('all_unique_symbols', [])) < 5:
                raise RuntimeError(
                    f"CRITICAL: TokenMappingManager returned insufficient tokens. "
                    f"Got {len(available_tokens.get('all_unique_symbols', []))} tokens, minimum 5 required for safe operations."
                )
            logger.info(f"âœ… TokenMappingManager validated: {len(available_tokens.get('all_unique_symbols', []))} tokens available")
            
        except ImportError:
            raise RuntimeError(
                "CRITICAL: TokenMappingManager not available. "
                "CoinGecko handler requires TokenMappingManager for dynamic token management."
            )
        except Exception as e:
            raise RuntimeError(
                f"CRITICAL: TokenMappingManager initialization failed: {e}. "
                f"CoinGecko handler cannot operate without dynamic token mappings."
            )
        
        # Initialize HTTP session with industry best practices
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        self.session = requests.Session()
        
        # Configure retry strategy with exponential backoff (Industry best practice)
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1
        )
        
        # Mount adapters with retry strategy
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Set default headers for all requests (Industry best practice)
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'DNT': '1',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'cross-site'
        })
        
        # Set session timeout defaults (Industry best practice)
        self.session_timeout = (10, 30)  # (connect_timeout, read_timeout)
        
        logger.info("ðŸš€ Enhanced CoinGecko Handler v3.0 initialized")
        logger.info(f"ðŸ“Š Cache duration: {cache_duration}s, Min interval: {self.min_request_interval}s")
        logger.info(f"ðŸ”§ HTTP session configured with retry strategy and connection pooling")
    
    def _initialize_token_mappings(self) -> None:
        """
        Initialize comprehensive token mappings using TokenMappingManager
        Zero fallback approach - relies entirely on TokenMappingManager's 300+ token system
        """
        try:
            # Import TokenMappingManager from config
            from config import TokenMappingManager
            
            # Initialize TokenMappingManager
            token_manager = TokenMappingManager()
            
            # Build common_mappings dynamically from TokenMappingManager
            self.common_mappings = {}
            
            # Get all available symbols from TokenMappingManager
            all_symbols = token_manager.get_all_symbols()
            
            if not all_symbols:
                raise RuntimeError(
                    "CRITICAL COINGECKO HANDLER FAILURE: "
                    "TokenMappingManager returned no symbols. Cannot initialize token mappings."
                )
            
            # Generate symbol-to-coingecko_id mappings for all tokens
            for symbol in all_symbols:
                try:
                    # Convert symbol to CoinGecko ID using TokenMappingManager
                    coingecko_id = token_manager.symbol_to_coingecko_id(symbol)
                    
                    if coingecko_id and coingecko_id != symbol.lower():
                        # Store as lowercase symbol -> coingecko_id for fast lookup
                        self.common_mappings[symbol.lower()] = coingecko_id
                        
                except Exception as e:
                    logger.warning(f"Failed to map symbol {symbol}: {e}")
                    continue
            
            # Log success metrics
            total_mappings = len(self.common_mappings)
            
            # Validate we have sufficient token coverage (no hardcoded list)
            if total_mappings < 50:
                raise RuntimeError(
                    f"CRITICAL COINGECKO HANDLER FAILURE: "
                    f"Insufficient token mappings generated: {total_mappings}. "
                    f"Expected 50+ tokens from TokenMappingManager for safe operation."
                )
            coingecko_ids = list(set(self.common_mappings.values()))
            unique_coingecko_ids = len(coingecko_ids)
            
            logger.info(f"âœ… Token mappings initialized successfully:")
            logger.info(f"   - Total symbol mappings: {total_mappings}")
            logger.info(f"   - Unique CoinGecko IDs: {unique_coingecko_ids}")
            logger.info(f"   - Source: TokenMappingManager (dynamic + 300-token fallback system)")
            
            # Debug log sample mappings for verification
            sample_mappings = dict(list(self.common_mappings.items())[:5])
            logger.debug(f"Sample mappings: {sample_mappings}")
            
        except ImportError as e:
            raise RuntimeError(
                f"CRITICAL COINGECKO HANDLER FAILURE: "
                f"Cannot import TokenMappingManager: {e}. "
                f"Ensure config.py is available and TokenMappingManager is properly defined."
            )
            
        except Exception as e:
            raise RuntimeError(
                f"CRITICAL COINGECKO HANDLER FAILURE: "
                f"TokenMappingManager initialization failed: {e}. "
                f"CoinGecko handler cannot operate without token mappings."
            )
    
    def _get_cache_key(self, endpoint: str, params: Dict) -> str:
        """Generate a unique cache key for the request"""
        # Sort params for consistent keys
        sorted_params = json.dumps(params, sort_keys=True)
        key_string = f"{endpoint}:{sorted_params}"
        # Use hash for shorter cache keys
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cache entry is still valid"""
        if cache_key not in self.cache:
            return False
        
        cache_entry = self.cache[cache_key]
        cache_time = cache_entry['timestamp']
        current_time = time.time()
        
        # Check if data quality is acceptable
        data_quality = cache_entry.get('quality_score', 1.0)
        if data_quality < 0.5:  # Don't use poor quality cached data
            return False
        
        return (current_time - cache_time) < self.cache_duration
    
    def _get_from_cache(self, cache_key: str) -> Any:
        """Get data from cache if available and valid"""
        if self._is_cache_valid(cache_key):
            cache_entry = self.cache[cache_key]
            logger.debug(f"Cache hit (quality: {cache_entry.get('quality_score', 1.0):.2f})")
            return cache_entry['data']
        return None
    
    def _add_to_cache(self, cache_key: str, data: Any, quality_score: float = 1.0) -> None:
        """Add data to cache with quality score"""
        self.cache[cache_key] = {
            'timestamp': time.time(),
            'data': data,
            'quality_score': quality_score
        }
        logger.debug(f"Cached data (quality: {quality_score:.2f})")
    
    def _clean_cache(self) -> None:
        """Remove expired cache entries and poor quality data"""
        current_time = time.time()
        expired_keys = []
        
        for key, entry in self.cache.items():
            # Remove if expired or poor quality
            is_expired = (current_time - entry['timestamp']) >= self.cache_duration
            is_poor_quality = entry.get('quality_score', 1.0) < 0.3
            
            if is_expired or is_poor_quality:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            logger.debug(f"Cleaned {len(expired_keys)} cache entries")
    
    def _enforce_rate_limit(self) -> None:
        """Enhanced rate limiting with adaptive delays"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        
        # Adaptive rate limiting based on recent failures
        adaptive_interval = self.min_request_interval
        if self.failed_requests > 5:
            adaptive_interval *= 2  # Slow down if many failures
        
        if time_since_last_request < adaptive_interval:
            sleep_time = adaptive_interval - time_since_last_request
            logger.debug(f"Rate limiting: {sleep_time:.2f}s (adaptive: {adaptive_interval:.2f}s)")
            time.sleep(sleep_time)
        
        # Reset daily counter if needed
        if safe_datetime_diff(datetime.now(), self.daily_requests_reset) >= 86400:
            self.daily_requests = 0
            self.failed_requests = 0
            self.successful_requests = 0
            self.daily_requests_reset = datetime.now()
            logger.info("ðŸ“Š Daily request counters reset")
    
    def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Any]:
        """Enhanced API request with better error handling"""
        if params is None:
            params = {}
        
        url = f"{self.base_url}/{endpoint}"
        self._enforce_rate_limit()
        
        self.last_request_time = time.time()
        self.daily_requests += 1
        self.data_quality_stats['total_requests'] += 1
        
        try:
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'application/json',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            logger.debug(f"ðŸŒ API request: {endpoint}")
            
            # Enhanced request with timeout and retry logic
            response = requests.get(
                url, 
                params=params, 
                headers=headers, 
                timeout=30,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                self.successful_requests += 1
                logger.debug(f"âœ… API success: {endpoint}")
                
                try:
                    data = response.json()
                    return data
                except json.JSONDecodeError as e:
                    logger.error(f"JSON decode error: {e}")
                    self.failed_requests += 1
                    return None
                    
            elif response.status_code == 429:
                self.failed_requests += 1
                logger.warning(f"âš ï¸ Rate limit hit: {response.status_code}")
                # Exponential backoff for rate limits
                backoff_time = self.retry_delay * (2 ** min(self.active_retries, 4))
                time.sleep(backoff_time)
                return None
                
            else:
                self.failed_requests += 1
                logger.error(f"âŒ API error: {response.status_code}")
                try:
                    error_detail = response.text[:200]  # First 200 chars
                    logger.error(f"Error detail: {error_detail}")
                except:
                    pass
                return None
                
        except requests.exceptions.Timeout:
            self.failed_requests += 1
            logger.error("â° Request timeout")
            return None
            
        except requests.exceptions.ConnectionError:
            self.failed_requests += 1
            logger.error("ðŸ”Œ Connection error")
            return None
            
        except Exception as e:
            self.failed_requests += 1
            logger.error(f"ðŸ’¥ Request exception: {str(e)}")
            return None
    
    def get_with_cache(self, endpoint: str, params: Optional[Dict] = None) -> Optional[Any]:
        """Enhanced caching with quality tracking"""
        if params is None:
            params = {}
        
        cache_key = self._get_cache_key(endpoint, params)
        
        # Try cache first
        cached_data = self._get_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        # Make API request with retries
        retry_count = 0
        self.active_retries = 0
        
        while retry_count < self.max_retries:
            self.active_retries = retry_count
            data = self._make_request(endpoint, params)
            
            if data is not None:
                # Calculate data quality score
                quality_score = self._assess_data_quality(data, endpoint)
                
                # Cache the data
                self._add_to_cache(cache_key, data, quality_score)
                
                logger.debug(f"âœ… Request successful after {retry_count} retries")
                return data
            
            retry_count += 1
            if retry_count < self.max_retries:
                retry_delay = self.retry_delay * (retry_count + 1)
                logger.warning(f"ðŸ”„ Retry {retry_count}/{self.max_retries} in {retry_delay}s")
                time.sleep(retry_delay)
        
        logger.error(f"âŒ Failed after {self.max_retries} retries: {endpoint}")
        return None
    
    def _assess_data_quality(self, data: Any, endpoint: str) -> float:
        """Assess the quality of received data - NO synthetic dependency"""
        try:
            quality_score = 1.0
            
            if not data:
                return 0.0
            
            # For market data, check real data availability (not sparkline dependent)
            if endpoint == "coins/markets" and isinstance(data, list):
                valid_entries = 0
                total_items = len(data)
                
                for item in data:
                    if isinstance(item, dict):
                        # Check for essential fields only (no sparkline requirement)
                        essential_fields = ['id', 'symbol', 'current_price', 'total_volume', 'market_cap']
                        has_essentials = all(field in item and item[field] is not None for field in essential_fields)
                        
                        if has_essentials:
                            valid_entries += 1
                
                if total_items > 0:
                    data_quality = valid_entries / total_items
                    quality_score = min(quality_score, data_quality)
            
            # Check for required fields
            if isinstance(data, list) and data:
                sample_item = data[0]
                if isinstance(sample_item, dict):
                    required_fields = ['id', 'symbol', 'current_price']
                    missing_fields = sum(1 for field in required_fields if field not in sample_item)
                    field_quality = 1.0 - (missing_fields / len(required_fields) * 0.5)
                    quality_score = min(quality_score, field_quality)
            
            return max(0.0, min(1.0, quality_score))
            
        except Exception as e:
            logger.debug(f"Quality assessment error: {e}")
            return 0.7  # Default moderate quality
    
    def get_market_data(self, params: Optional[Dict] = None) -> Optional[List[Dict[str, Any]]]:
        """
        Market data retrieval that works WITHOUT sparkline dependencies
        FIXED: Pylance errors for is_valid and price_array variables
        
        Args:
            params: Query parameters for the API
        
        Returns:
            List of market data entries (with sparklines only if available from API)
        """
        endpoint = "coins/markets"
        
        # Default params - sparkline requested but not required
        if params is None:
            params = {
                "vs_currency": "usd",
                "ids": "bitcoin,ethereum,solana,ripple,binancecoin,avalanche-2,polkadot,uniswap,near,aave,polygon-pos,filecoin,kaito,official-trump",
                "order": "market_cap_desc",
                "per_page": 100,
                "page": 1,
                "sparkline": True,  # Request but don't require
                "price_change_percentage": "1h,24h,7d,30d",
                "include_market_cap": True,
                "include_24hr_vol": True,
                "include_24hr_change": True
            }
        
        raw_result = self.get_with_cache(endpoint, params)
        
        if not raw_result:
            logger.error("âŒ No raw data from CoinGecko API")
            return None
        
        # Handle string result
        if isinstance(raw_result, str):
            try:
                raw_result = json.loads(raw_result)
            except Exception as e:
                logger.error(f"Failed to parse string result: {e}")
                return None
        
        if not isinstance(raw_result, list):
            logger.error(f"Unexpected data format: {type(raw_result)}")
            return None
        
        # Process data without requiring sparklines
        validated_data = []
        tokens_with_sparklines = 0
        
        for item in raw_result:
            try:
                if not isinstance(item, dict):
                    continue
                
                # Extract essential data (sparklines optional)
                symbol = item.get('symbol', '').upper()
                current_price = item.get('current_price')
                
                if not symbol or not current_price:
                    logger.warning(f"Missing essential data for item: {item.get('id', 'unknown')}")
                    continue
                
                # Initialize variables to prevent Pylance "possibly unbound" errors
                is_valid = False
                price_array = []
                
                # Handle sparkline data if available (optional)
                sparkline_data = item.get('sparkline_in_7d')
                if sparkline_data:
                    is_valid, price_array = self.validator.validate_sparkline_data(sparkline_data)
                    if is_valid:
                        item['sparkline_in_7d'] = {'price': price_array}
                        tokens_with_sparklines += 1
                    else:
                        # Remove invalid sparkline
                        item['sparkline_in_7d'] = {'price': []}
                else:
                    # No sparkline data from API
                    item['sparkline_in_7d'] = {'price': []}
                
                # Add data quality indicators
                # Now is_valid and price_array are always defined
                item['_data_quality'] = {
                    'sparkline_available': is_valid,
                    'sparkline_points': len(price_array) if is_valid else 0,
                    'data_timestamp': time.time()
                }
                
                validated_data.append(item)
                
            except Exception as e:
                logger.error(f"Error processing item {item.get('id', 'unknown')}: {e}")
                continue
        
        # Log results (no synthetic data tracking)
        total_processed = len(validated_data)
        if total_processed > 0:
            sparkline_rate = (tokens_with_sparklines / total_processed) * 100
            logger.info(f"ðŸ“Š Processed {total_processed} tokens, {tokens_with_sparklines} with real sparklines ({sparkline_rate:.1f}%)")
        
        return validated_data if validated_data else None
    
    def get_market_data_batched(self, token_ids: List[str], batch_size: int = 50) -> Optional[List[Dict[str, Any]]]:
        """
        Enhanced batched market data retrieval
        
        Args:
            token_ids: List of CoinGecko token IDs
            batch_size: Maximum number of tokens per request
            
        Returns:
            Combined list of validated market data entries
        """
        if not token_ids:
            logger.warning("No token IDs provided for batched request")
            return []
        
        all_data = []
        successful_batches = 0
        
        for i in range(0, len(token_ids), batch_size):
            batch = token_ids[i:i+batch_size]
            batch_ids = ','.join(batch)
            
            params = {
                "vs_currency": "usd",
                "ids": batch_ids,
                "order": "market_cap_desc",
                "per_page": len(batch),
                "page": 1,
                "sparkline": True,
                "price_change_percentage": "1h,24h,7d,30d"
            }
            
            logger.debug(f"ðŸ“¦ Processing batch {i//batch_size + 1}: {len(batch)} tokens")
            
            batch_data = self.get_market_data(params)
            if batch_data:
                all_data.extend(batch_data)
                successful_batches += 1
                logger.debug(f"âœ… Batch {i//batch_size + 1} successful: {len(batch_data)} items")
            else:
                logger.error(f"âŒ Batch {i//batch_size + 1} failed")
        
        total_batches = (len(token_ids) + batch_size - 1) // batch_size
        success_rate = (successful_batches / total_batches) * 100 if total_batches > 0 else 0
        
        logger.info(f"ðŸ“Š Batched request complete: {successful_batches}/{total_batches} batches ({success_rate:.1f}%)")
        
        return all_data if all_data else None
    
    def find_token_id(self, token_symbol: str) -> Optional[str]:
        """
        Enhanced token ID lookup with better caching
        
        Args:
            token_symbol: Token symbol (e.g., 'BTC', 'ETH')
            
        Returns:
            CoinGecko ID for token or None if not found
        """
        token_symbol_lower = token_symbol.lower()
        
        # Check common mappings first (fastest)
        if token_symbol_lower in self.common_mappings:
            logger.debug(f"ðŸŽ¯ Quick lookup: {token_symbol} -> {self.common_mappings[token_symbol_lower]}")
            return self.common_mappings[token_symbol_lower]
        
        # Check cache
        if token_symbol_lower in self.token_id_cache:
            cached_id = self.token_id_cache[token_symbol_lower]
            logger.debug(f"ðŸ“‹ Cache hit: {token_symbol} -> {cached_id}")
            return cached_id
        
        # Fetch from API as last resort
        logger.info(f"ðŸ” Looking up {token_symbol} from CoinGecko API")
        
        endpoint = "coins/list"
        coins_list = self.get_with_cache(endpoint)
        
        if not coins_list:
            logger.error(f"Failed to fetch coins list for {token_symbol}")
            return None
        
        # Search for exact symbol match first
        for coin in coins_list:
            if coin.get('symbol', '').lower() == token_symbol_lower:
                coin_id = coin['id']
                logger.info(f"âœ… Found {token_symbol} -> {coin_id}")
                # Cache the result
                self.token_id_cache[token_symbol_lower] = coin_id
                return coin_id
        
        # Fallback: partial name match
        for coin in coins_list:
            coin_name = coin.get('name', '').lower()
            if token_symbol_lower in coin_name:
                coin_id = coin['id']
                logger.info(f"ðŸ“ Partial match {token_symbol} -> {coin_id} ({coin['name']})")
                # Cache with lower confidence
                self.token_id_cache[token_symbol_lower] = coin_id
                return coin_id
        
        logger.warning(f"âŒ Could not find {token_symbol} in CoinGecko")
        return None
    
    def get_multiple_tokens_by_symbol(self, symbols: List[str]) -> Dict[str, str]:
        """
        Enhanced bulk token ID lookup
        
        Args:
            symbols: List of token symbols
            
        Returns:
            Dictionary mapping symbols to CoinGecko IDs
        """
        result = {}
        symbols_to_fetch = []
        
        # Quick lookups first
        for symbol in symbols:
            symbol_lower = symbol.lower()
            
            # Check common mappings
            if symbol_lower in self.common_mappings:
                result[symbol] = self.common_mappings[symbol_lower]
                continue
            
            # Check cache
            if symbol_lower in self.token_id_cache:
                result[symbol] = self.token_id_cache[symbol_lower]
                continue
            
            # Need API lookup
            symbols_to_fetch.append(symbol)
        
        logger.debug(f"ðŸŽ¯ Quick lookups: {len(result)}/{len(symbols)}, API needed: {len(symbols_to_fetch)}")
        
        if not symbols_to_fetch:
            return result
        
        # Batch API lookup for remaining symbols
        endpoint = "coins/list"
        coins_list = self.get_with_cache(endpoint)
        
        if not coins_list:
            logger.error("Failed to fetch coins list for bulk lookup")
            # Add None entries for failed lookups
            for symbol in symbols_to_fetch:
                result[symbol] = None
            return result
        
        # Create efficient lookup dictionary
        symbol_to_id = {}
        for coin in coins_list:
            coin_symbol = coin.get('symbol', '').lower()
            if coin_symbol and coin_symbol not in symbol_to_id:
                symbol_to_id[coin_symbol] = coin['id']
        
        # Process remaining symbols
        found_count = 0
        for symbol in symbols_to_fetch:
            symbol_lower = symbol.lower()
            if symbol_lower in symbol_to_id:
                coin_id = symbol_to_id[symbol_lower]
                result[symbol] = coin_id
                self.token_id_cache[symbol_lower] = coin_id
                found_count += 1
                logger.debug(f"âœ… API lookup: {symbol} -> {coin_id}")
            else:
                result[symbol] = None
                logger.warning(f"âŒ Not found: {symbol}")
        
        logger.info(f"ðŸ“Š Bulk lookup complete: {found_count}/{len(symbols_to_fetch)} found via API")
        
        return result
    
    def get_coin_detail(self, coin_id: str) -> Optional[Dict[str, Any]]:
        """Enhanced coin detail retrieval"""
        endpoint = f"coins/{coin_id}"
        params = {
            "localization": "false",
            "tickers": "false",
            "market_data": "true",
            "community_data": "false",
            "developer_data": "false",
            "sparkline": "true"
        }
        
        return self.get_with_cache(endpoint, params)
    
    def get_coin_ohlc(self, coin_id: str, days: int = 1) -> Optional[List[List[float]]]:
        """Enhanced OHLC data retrieval with validation"""
        # Validate days parameter
        valid_days = [1, 7, 14, 30, 90, 180, 365]
        if days not in valid_days:
            logger.warning(f"Invalid days value {days}, using closest valid value")
            days = min(valid_days, key=lambda x: abs(x - days))
        
        endpoint = f"coins/{coin_id}/ohlc"
        params = {
            "vs_currency": "usd",
            "days": days
        }
        
        ohlc_data = self.get_with_cache(endpoint, params)
        
        if not ohlc_data:
            return None
        
        # Validate OHLC data format
        validated_ohlc = []
        for entry in ohlc_data:
            try:
                if isinstance(entry, list) and len(entry) >= 5:
                    # Ensure all values are valid numbers
                    timestamp, open_price, high, low, close = entry[:5]
                    if all(isinstance(x, (int, float)) and math.isfinite(x) for x in [open_price, high, low, close]):
                        validated_ohlc.append([float(timestamp), float(open_price), float(high), float(low), float(close)])
            except Exception as e:
                logger.debug(f"Invalid OHLC entry: {e}")
                continue
        
        return validated_ohlc if validated_ohlc else None
    
    def get_trending_tokens(self) -> Optional[List[Dict[str, Any]]]:
        """Enhanced trending tokens retrieval"""
        endpoint = "search/trending"
        result = self.get_with_cache(endpoint)
        
        if result and 'coins' in result:
            trending_coins = result['coins']
            
            # Extract and validate trending coin data
            validated_trending = []
            for coin_entry in trending_coins:
                try:
                    if isinstance(coin_entry, dict) and 'item' in coin_entry:
                        coin_data = coin_entry['item']
                        if isinstance(coin_data, dict) and 'id' in coin_data:
                            validated_trending.append(coin_data)
                except Exception as e:
                    logger.debug(f"Invalid trending entry: {e}")
                    continue
            
            logger.info(f"ðŸ“ˆ Retrieved {len(validated_trending)} trending tokens")
            return validated_trending
        
        return None
    
    def check_token_exists(self, token_id: str) -> bool:
        """Enhanced token existence check"""
        endpoint = f"coins/{token_id}"
        params = {
            "localization": "false",
            "tickers": "false", 
            "market_data": "false",
            "community_data": "false",
            "developer_data": "false",
            "sparkline": "false"
        }
        
        try:
            url = f"{self.base_url}/{endpoint}"
            self._enforce_rate_limit()
            
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'application/json'
            }
            
            response = requests.get(url, params=params, headers=headers, timeout=10)
            self.last_request_time = time.time()
            self.daily_requests += 1
            
            exists = response.status_code == 200
            logger.debug(f"Token existence check for {token_id}: {'âœ…' if exists else 'âŒ'}")
            return exists
            
        except Exception as e:
            logger.error(f"Error checking token existence for {token_id}: {e}")
            return False
    
    def optimize_for_multiple_tokens(self, tokens: List[str]) -> bool:
        """Enhanced optimization for multiple tokens"""
        try:
            logger.info(f"ðŸ”§ Optimizing handler for {len(tokens)} tokens")
            
            # Pre-fetch and cache token IDs
            token_ids = self.get_multiple_tokens_by_symbol(tokens)
            valid_ids = [id for id in token_ids.values() if id]
            
            if not valid_ids:
                logger.warning("No valid token IDs found for optimization")
                return False
            
            # Pre-fetch market data in batches
            market_data = self.get_market_data_batched(valid_ids)
            
            if market_data:
                cached_count = len(market_data)
                logger.info(f"âœ… Pre-cached market data for {cached_count} tokens")
                
                # Update data quality stats
                self.data_quality_stats['optimization_runs'] = self.data_quality_stats.get('optimization_runs', 0) + 1
                
                return True
            else:
                logger.warning("Failed to pre-cache market data")
                return False
                
        except Exception as e:
            logger.error(f"Optimization failed: {e}")
            return False
    
    # ============================================================================
    # ðŸ“Š MONITORING AND DIAGNOSTICS ðŸ“Š
    # ============================================================================
    
    def get_request_stats(self) -> Dict[str, Any]:
        """Enhanced request statistics - NO synthetic data tracking"""
        self._clean_cache()
        
        # Calculate success rate
        total_requests = self.successful_requests + self.failed_requests
        success_rate = (self.successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        stats = {
            # Request statistics
            'daily_requests': self.daily_requests,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'success_rate_percent': round(success_rate, 2),
            
            # Cache statistics
            'cache_size': len(self.cache),
            'token_id_cache_size': len(self.token_id_cache),
            
            # Data quality statistics (real data only)
            'data_quality': {
                'total_processed': self.data_quality_stats.get('total_requests', 0),
                'valid_entries': self.data_quality_stats.get('valid_sparklines', 0),
                'failed_validations': self.data_quality_stats.get('failed_validations', 0)
            },
            
            # Performance metrics
            'performance': {
                'min_request_interval': self.min_request_interval,
                'cache_duration': self.cache_duration,
                'active_retries': self.active_retries,
                'max_retries': self.max_retries
            }
        }
        
        return stats
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get comprehensive handler health status"""
        stats = self.get_request_stats()
        
        # Determine health status
        success_rate = stats['success_rate_percent']
        synthetic_rate = stats['data_quality']['synthetic_rate_percent']
        
        if success_rate >= 95 and synthetic_rate <= 20:
            health = "excellent"
        elif success_rate >= 80 and synthetic_rate <= 50:
            health = "good"
        elif success_rate >= 60:
            health = "fair"
        else:
            health = "poor"
        
        # Health recommendations
        recommendations = []
        if success_rate < 80:
            recommendations.append("Check network connectivity and API key")
        if synthetic_rate > 50:
            recommendations.append("High synthetic data usage - check CoinGecko API status")
        if self.failed_requests > 20:
            recommendations.append("Consider increasing retry delays")
        
        return {
            'health_status': health,
            'success_rate': success_rate,
            'synthetic_data_rate': synthetic_rate,
            'recommendations': recommendations,
            'last_check': datetime.now().isoformat(),
            'detailed_stats': stats
        }
    
    def reset_stats(self) -> None:
        """Reset all statistics and counters"""
        self.daily_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.active_retries = 0
        self.daily_requests_reset = datetime.now()
        
        # Reset data quality stats
        self.data_quality_stats = {
            'valid_sparklines': 0,
            'synthetic_sparklines': 0,
            'failed_validations': 0,
            'total_requests': 0
        }
        
        logger.info("ðŸ“Š All statistics reset")
    
    def clear_cache(self) -> None:
        """Clear all cached data"""
        cache_size = len(self.cache)
        token_cache_size = len(self.token_id_cache)
        
        self.cache.clear()
        self.token_id_cache.clear()
        
        logger.info(f"ðŸ—‘ï¸ Cache cleared: {cache_size} entries, {token_cache_size} token mappings")
    
    # ============================================================================
    # ðŸŽ¯ COMPATIBILITY METHODS ðŸŽ¯
    # ============================================================================
    
    def validate_and_enhance_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Validate market data using REAL data only - NO synthetic enhancement
        FIXED: Pylance errors for is_valid and price_array variables
        
        Args:
            raw_data: Raw market data from API
        
        Returns:
            List of validated market data with real sparklines only
        """
        if not raw_data or not isinstance(raw_data, list):
            return []
        
        validated_data = []
        real_sparklines = 0
        missing_sparklines = 0
        
        for item in raw_data:
            try:
                if not isinstance(item, dict):
                    continue
                
                # Initialize variables to prevent Pylance "possibly unbound" errors
                is_valid = False
                price_array = []
                
                # Validate sparkline if present - REAL data only
                sparkline_data = item.get('sparkline_in_7d')
                if sparkline_data:
                    is_valid, price_array = self.validator.validate_sparkline_data(sparkline_data)
                    
                    if is_valid:
                        # Use REAL validated sparkline data
                        item['sparkline_in_7d'] = {'price': price_array}
                        real_sparklines += 1
                    else:
                        # Remove invalid sparkline data - NO synthetic replacement
                        item['sparkline_in_7d'] = {'price': []}
                        missing_sparklines += 1
                else:
                    # No sparkline data available - leave empty
                    item['sparkline_in_7d'] = {'price': []}
                    missing_sparklines += 1
                
                # Add data quality indicators (no synthetic tracking)
                # Now is_valid and price_array are always defined
                item['_data_quality'] = {
                    'sparkline_available': is_valid,
                    'sparkline_points': len(price_array) if is_valid else 0,
                    'data_timestamp': time.time()
                }
                
                validated_data.append(item)
                
            except Exception as e:
                logger.debug(f"Error processing item {item.get('id', 'unknown')}: {e}")
                continue
        
        # Log data quality summary (real data only)
        total_processed = len(validated_data)
        if total_processed > 0:
            real_rate = (real_sparklines / total_processed) * 100
            logger.info(f"ðŸ“Š Processed {total_processed} tokens, {real_sparklines} real sparklines ({real_rate:.1f}%), {missing_sparklines} missing")
        
        return validated_data
    
    def __str__(self) -> str:
        """String representation of handler status"""
        stats = self.get_request_stats()
        return (f"CoinGeckoHandler("
                f"requests={stats['daily_requests']}, "
                f"success_rate={stats['success_rate_percent']:.1f}%, "
                f"cache_size={stats['cache_size']}, "
                f"synthetic_rate={stats['data_quality']['synthetic_rate_percent']:.1f}%)")
    
    def __repr__(self) -> str:
        """Detailed representation for debugging"""
        return (f"CoinGeckoHandler(base_url='{self.base_url}', "
                f"cache_duration={self.cache_duration}, "
                f"min_interval={self.min_request_interval})")

# ============================================================================
# ðŸš€ MODULE COMPLETION AND EXPORTS ðŸš€
# ============================================================================

# Export the enhanced handler class
__all__ = [
    'CoinGeckoHandler',
    'DataQualityValidator'
]

# Module information
__version__ = "2.0.0"
__author__ = "Enhanced CoinGecko Integration System"
__description__ = "Next-generation CoinGecko API handler with advanced data validation"

# Log module completion
logger.info("ðŸš€ Enhanced CoinGecko Handler v3.0 module loaded successfully")
logger.info("âœ… Features: Advanced validation, synthetic data, M4 optimization, monitoring")
logger.info("ðŸ”§ Backward compatible with existing bot.py and prediction_engine.py")

# End of enhanced_coingecko_handler.py